# Introduction  
- 경영관리 분야에서 재고관리 시스템에 관한 연구가 과거 수십년동안 이루어졌다.
- 그 목적은 주로 운영 비용을 최소화하기 위해 주문 결정을 제어하는 것이었다.
- 또한 현재의 결정이 이후 기간동안의 재고에 영향을 미치기 때문에 sequential decision-making problem으로 여겨졌다.
- 이런한 문제들은 MDP 문제로 공식화되어 일반적으로 policy 기반의 해법으로 해결된다.
- 과거에 이와 관련된 연구들이 많이 이루어졌는데 대표적인 게 (s,S) 정책이다.
- (s,S)정책은 재주문 수준 s, 최대 주문 수준 S를 parameter로써 가지며 setup cost가 존재하는 하의 optimal한 정책이다.
- 이 정책은 현재 재고가 s 이하로 떨어지면 S 까지 보충하는 정책이다.

- 이 연구에선 multi-item 재고 보충 문제를 다뤘다.
- 기존 연구를 차용하기 위해 본 문제를 instantaneous 보충 환경을 가정한다.
- inventory system은 정해진 시간에 주기적으로 재고 수준을 확인하며 setup cost가 존재한다.
- single-item인 경우엔 (s,S) policy가 optimal한 것으로 여겨지지만 이렇게 multi-item인 경우엔 (optimal policy는 아직 없지만) (s,c,S) joint repleshment policy가 적용가능한 방법으로 간주된다.
- (s,c,S)정책은 reorder, can-order, order up-to level 이렇게 3개의 파라미터를 가진다.
- 한 품목이라도 재고수준이 s 이하로 떨어지면, 재고 수준이 c보다 낮은 품목을 모두 S까지 주문하는 방식이다.

- 재고 보충 문제는 수요의 분포를 미리 알고 있고, 그것이 안정적이라는 가정 하에 발전했다.
- 이는 수요가 변화할 때마다 최적의 정책 또한 update되어야 한다는 것의 의미한다.
- 그러나 현실에선 이런 수요의 정도를 예상할 수 없으며 regime switch와 같은 불안정한 특성을 보인다.
- 이런 특성 때문에 수요의 변화를 감지하는 것, policy를 update하는 것을 늦춘다.
- 더욱이 dynamic programming과 같은 전통적인 방법들은 방대한 계산량 때문에 이런 변화들에 빠르게 반응할 수 없다.
- 이런 한계를 극복하고자 unknown, non-stationary nature에 대응할 data-driven 접근들이 이루어졌다.

- 그 중의 하나가 RL을 이용한 접근이다.
- 최근의 RL 방법론들은 neural network와 같은 매개함수를 이용하여 policy나 value 함수를 근사한다.
- RL이 수요 distribution을 자동으로 알아차리고 regime switching에 정책을 적응하기 때문에 완전한 정보가 없어도 거의 optimal한 policy를 찾을 수 있었다.
- 사전 수요 데이터를 필요로 하지 않으며 관찰되는 수요를 실시간으로 학습하고 최적화할 수 있다.
- 그러나 치명적인 단점으로 높은 계산량과 느린 수렴 속도가 있다. 그래서 큰 사이즈의 재고 시스템에서는 변화하는 환경에 policy를 적응하는 데 실패했고 비효율적으로 됐다.

- 이 논문에선 기존의 좋은 policy의 구조적 특성을 차용하여 학습 성능을 개선하는 RL 방법을 제안한다.
- 예를들어, 1960년에 Scarf는 (s,S) 정책의 optimality를 증명하기 위해 K-convexity를 제안했는데, 이 K-convexity를 이용할 것이다.
- 또한 action value function을 학습하는 대신 (s,S) policy를 바로 update할 것이다.
- 그러면 수요의 변화를 즉시 포착해 그 변화하는 수요에 replenish policy를 바로 적응(변화)시킨다.
- 우리는 이 방법이 single, multi-item problem에서 거의 optimal한 policy를 제공한다는 것을 보일 뿐만 아니라 소매 산업에서 기존보다 운영 효율성이 향상됐다는 것을 보였다.

그래서 이 논문을 요약하자면   
 * 수요 분포를 사전에 알 필요 없이 재고 정책을 최적화하는 RL 알고리즘을 발전시켰다.
 * 이 RL 알고리즘은 변화하는 수요에 맞춰 update가 가능하며 운영적 효율성을 제고시킨다.
 * item의 크기가 커져도, 수요 분포가 다양해져도 거의 optimal한 policy를 얻을 수 있다.
 * 이 방법은 기존의 좋은 policy의 구조적 특성을 차용해 높은 알고리즘적 효율성과 수렴 비율을 보인다.

***

